{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Rough Draft</h5>\n",
    "\n",
    "Types of models to use :\n",
    "* Linear Regression (Check if underfit with plots)\n",
    "    - Log transform skewed distributions\n",
    "* KNN\n",
    "* XGBoost\n",
    "* https://automl.github.io/auto-sklearn/master/\n",
    "\n",
    "How to finetune model:\n",
    "* param grid tuning\n",
    "* Cross Validation\n",
    "* K Best features\n",
    "* R-squared, Adjusted R-squared\n",
    "\n",
    "How to interpret model:\n",
    "* Feature Importance\n",
    "* Permutation Importance\n",
    "* SHAP values and plots\n",
    "* Partial Dependence Plots (1 and 2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupKFold (K = 4, Group by country, then remove country)\n",
    "\n",
    "Pipeline (Scale, PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/radius3_selected_features.csv',index_col=[0])\n",
    "X = df.drop(['Gini index','Country Name'],axis=1).to_numpy()\n",
    "\n",
    "y = df['Gini index'].to_numpy()\n",
    "groups = df['Country Name'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Preprocessing</h5>\n",
    "Below I set up the initial pipeline that preprocesses the data and splits it into folds.\n",
    "\n",
    "* since values between countries were imputed, Group K fold seperates the data and makes sure that countries don't overlap.\n",
    "* Each fold is then standard scaled with the rest of the folds\n",
    "* In a similar way I use PCA to reduce the 6 poverty features into 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "preprocess_steps = [('scaler',StandardScaler()),\n",
    "                    ('ct',ColumnTransformer([('pca',PCA(n_components=2),list(range(13,19)))],remainder='passthrough'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "Pipeline(steps=preprocess_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Linear models</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "\n",
    "\n",
    "ols = Pipeline(steps=preprocess_steps+[('ols',LinearRegression())])\n",
    "test_scores = cross_val_score(ols,X,y,groups=groups,cv=gkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cross_val_predict(ols,X,y,groups=groups,cv=gkf)\n",
    "\n",
    "sns.jointplot(x=preds,y=preds-y)\n",
    "plt.xlabel('Predicted Gini Index')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's correlation in the residuals and Gini Index isn't normally distributed, Linear regression (or atleast ordinary least squares) isn't a good fit for the data.\n",
    "\n",
    "* I next tried LassoLars which includes a regularization parameter to prevent overfitting and increases coefficents of features in a forward stepwise manner. \n",
    "* Although it doesn't change the fact that the data doesn't fit a linear model, it's coefficients give good insight into which features are most important and the direction of that relationship. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "lars = Pipeline(steps=preprocess_steps+[('lars',LassoLars())])\n",
    "\n",
    "test_scores = cross_val_score(lars,X,y,groups=groups,cv=gkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.drop(['Country Name', 'Gini index'],axis=1).columns.to_list()\n",
    "feature_names = [col for col in columns if col.find('Poverty') < 0]\n",
    "\n",
    "feature_names = ['poverty_pca0','poverty_pca1'] + feature_names #the column transformer outputs features in this order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = gkf.split(X,y,groups=groups)\n",
    "coefs = {}\n",
    "for f in feature_names:\n",
    "    coefs[f] = []\n",
    "for i in range(5):\n",
    "    train_ind, test_ind = next(folds)\n",
    "    X_train, y_train, X_test, y_test = X[train_ind], y[train_ind], X[test_ind], y[test_ind]\n",
    "    lars.fit(X_train,y_train)\n",
    "    fold_coefs = lars.named_steps['lars'].coef_\n",
    "    \n",
    "    for j in range(len(feature_names)):\n",
    "        coefs[feature_names[j]].append(fold_coefs[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(pd.DataFrame(coefs),orient='h')\n",
    "plt.title('LassoLars coefficients');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the variables are standard scaled, The coefficients can compared directly.\n",
    "\n",
    "According to lasso the most important variables in order are:\n",
    "- Educational attainment, at least completed lower secondary, population 25+, total (%) (cumulative)\n",
    "- Cause of death, by injury (% of total)\n",
    "- Hospital beds (per 1,000 people)  \n",
    "- GDP per capita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>K Neighbors Regression</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "KNN = Pipeline(steps=preprocess_steps+[('knn',KNeighborsRegressor())])\n",
    "\n",
    "test_scores = cross_val_score(KNN,X,y,groups=groups,cv=gkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "avg_train_scores = []\n",
    "avg_test_scores = []\n",
    "nn_range = range(2,20)\n",
    "\n",
    "for nn in nn_range:\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    knn = KNN.set_params(knn__n_neighbors=nn)\n",
    "    scores = cross_validate(knn,X,y,groups=groups,cv=gkf,return_train_score=True)\n",
    "    \n",
    "    avg_train_scores.append(np.average(scores['train_score']))\n",
    "    avg_test_scores.append(np.average(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn_range,avg_train_scores) # type: ignore\n",
    "plt.plot(nn_range,avg_test_scores) # type: ignore\n",
    "plt.grid(True)\n",
    "plt.xlabel('number of neighbors')\n",
    "plt.ylabel('R-squared')\n",
    "plt.legend({'train':'blue','test':'orange'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "avg_train_scores = []\n",
    "avg_test_scores = []\n",
    "\n",
    "pipeline = Pipeline(steps=preprocess_steps+[('skb',SelectKBest()),('knn',KNeighborsRegressor())])\n",
    "\n",
    "for k_val in range(1,16):\n",
    "    \n",
    "    model = pipeline.set_params(skb__k=k_val,skb__score_func=mutual_info_regression,knn__n_neighbors=8)\n",
    "\n",
    "    scores = cross_validate(model,X,y,groups=groups,cv=gkf,return_train_score=True)\n",
    "    \n",
    "    avg_train_scores.append(np.average(scores['train_score']))\n",
    "    avg_test_scores.append(np.average(scores['test_score']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,16),avg_train_scores) # type: ignore\n",
    "plt.plot(range(1,16),avg_test_scores) # type: ignore\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"'k' best features\")\n",
    "plt.ylabel('R-squared')\n",
    "plt.legend({'train':'blue','test':'orange'})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dip in the middle shows that select K best features may not be selecting optimally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I run a grid search to tune the parameters for K nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import f_regression,r_regression,mutual_info_regression\n",
    "\n",
    "pipeline = Pipeline(steps=preprocess_steps+[('skb',SelectKBest()),('knn',KNeighborsRegressor())])\n",
    "\n",
    "params = {\n",
    "    'skb__k':range(1,16),                                                 #how many features to select\n",
    "    'skb__score_func':[mutual_info_regression,f_regression,r_regression], #how to select features\n",
    "    'knn__n_neighbors':range(5,15),                                       #number of neighbors\n",
    "    'knn__weights':['uniform','distance'],                                #weigh all neighbors uniformly or by distance\n",
    "    'knn__p':[1,2,3]                                                      # how to measure distance, manhattan, euclidean, etc.\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline,params,cv=gkf)\n",
    "\n",
    "grid_search.fit(X,y,groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best KNN model explains about 60% of the variance and uses 15 of the 16 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb = SelectKBest(score_func=mutual_info_regression,k=15)\n",
    "\n",
    "pipe = Pipeline(preprocess_steps)\n",
    "\n",
    "X_transformed = pipe.fit_transform(X,y)\n",
    "\n",
    "skb.fit(X_transformed,y)\n",
    "best_features=skb.get_feature_names_out(feature_names) # type: ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df.columns) - set(best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature it omits is Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>XG Boost</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = Pipeline(preprocess_steps + [('xgb',XGBRegressor())])\n",
    "\n",
    "test_scores = cross_val_score(xgb,X,y,groups=groups,cv=gkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = gkf.split(X,y,groups=groups)\n",
    "feature_importances = {}\n",
    "for f in feature_names:\n",
    "    feature_importances[f] = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_ind, test_ind = next(folds)\n",
    "    X_train, y_train, X_test, y_test = X[train_ind], y[train_ind], X[test_ind], y[test_ind]\n",
    "    xgb.fit(X_train,y_train)\n",
    "\n",
    "    fold_fi = xgb.named_steps['xgb'].feature_importances_\n",
    "    \n",
    "    for j in range(len(feature_names)):\n",
    "        feature_importances[feature_names[j]].append(fold_fi[j])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(pd.DataFrame(feature_importances),orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just like the lassolars model, XGBoost also indicates that Hospital beds per 1000 people, Cause of death by injury and lower secondary education as the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'xgb__n_estimators':np.arange(5,150,5),\n",
    "    'xgb__max_depth':np.arange(2,9),\n",
    "    'xgb__learning_rate':np.arange(0,1,0.2),\n",
    "    'xgb__reg_lambda':np.arange(0,2,0.4),\n",
    "    'xgb__reg_alpha':np.arange(0,1,0.2)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb,param_distributions=param_grid,cv=gkf,n_iter=450)\n",
    "\n",
    "random_search.fit(X,y,groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>K Neighbors vs XG Boost</h5>\n",
    "\n",
    "- KNN scores a bit better with a score of 0.59 vs 0.56\n",
    "- xgboost model is more interpretable as we can look at the feature importance.\n",
    "- While KNN is less computationally expensive with training, it is more expensive for prediction since it requires storing the entire dataset \n",
    "\n",
    "Let's see if the residuals between both models can highlight any differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds = cross_val_predict(grid_search.best_estimator_,X,y,groups=groups,cv=gkf)\n",
    "\n",
    "sns.jointplot(x=knn_preds,y=knn_preds-y)\n",
    "plt.xlabel('predicted Gini Index')\n",
    "plt.ylabel('KNN Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds = cross_val_predict(random_search.best_estimator_,X,y,groups=groups,cv=gkf)\n",
    "\n",
    "sns.jointplot(x=xgb_preds,y=xgb_preds-y)\n",
    "plt.xlabel('predicted Gini Index')\n",
    "plt.ylabel('XGBoost Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Models have similar residual graphs but XGBoosts residuals seem to be centered a bit lower than 0\n",
    "\n",
    "another way to decide is to see which model performs better with less features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "xgb_params = random_search.best_params_\n",
    "xgb_rfe = Pipeline(preprocess_steps + [('rfe',RFE(XGBRegressor())),('xgb',XGBRegressor())])\n",
    "\n",
    "knn_params = grid_search.best_params_\n",
    "del knn_params['skb__k']\n",
    "del knn_params['skb__score_func']\n",
    "best_knn = KNeighborsRegressor(n_neighbors=12,p=1,weights='uniform')\n",
    "knn_sfs = Pipeline(preprocess_steps + [('sfs',SequentialFeatureSelector(best_knn)),('knn',KNeighborsRegressor())])\n",
    "\n",
    "xgb_test_scores = []\n",
    "knn_test_scores = []\n",
    "\n",
    "for i in range(1,len(feature_names)):\n",
    "    knn_model = knn_sfs.set_params(sfs__n_features_to_select=i,**knn_params)\n",
    "    xgb_model = xgb_rfe.set_params(rfe__n_features_to_select=i,**xgb_params)\n",
    "\n",
    "    xgb_test_scores.append(np.average(cross_val_score(xgb_model,X,y,groups=groups,cv=gkf)))\n",
    "    knn_test_scores.append(np.average(cross_val_score(knn_model,X,y,groups=groups,cv=gkf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(feature_names)),xgb_test_scores) #type: ignore\n",
    "plt.plot(range(1,len(feature_names)),knn_test_scores) #type: ignore\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('R-squared')\n",
    "plt.grid(True)\n",
    "plt.legend({'xgb':'blue','knn':'orange'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like xgb is better with less features so that's the model I'll go with\n",
    "\n",
    "Let's see which features we can drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe_params = {'reg_lambda': 1.2000000000000002,\n",
    " 'reg_alpha': 0.2,\n",
    " 'n_estimators': 115,\n",
    " 'max_depth': 2,\n",
    " 'learning_rate': 0.4}\n",
    "\n",
    "folds = gkf.split(X,y,groups=groups)\n",
    "scores = []\n",
    "chosen_features = {}\n",
    "for f in feature_names:\n",
    "    chosen_features[f] = 0\n",
    "\n",
    "for i in range(5):\n",
    "    train_ind, test_ind = next(folds)\n",
    "    X_train, y_train, X_test, y_test = X[train_ind], y[train_ind], X[test_ind], y[test_ind]\n",
    "    preprocessor = Pipeline(steps=preprocess_steps)\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    rfe = RFE(XGBRegressor(**rfe_params),n_features_to_select=10)\n",
    "    rfe.fit(X_train,y_train)\n",
    "    scores.append(rfe.score(X_test,y_test))\n",
    "    selected = rfe.get_feature_names_out(feature_names) # type: ignore\n",
    "    for f in feature_names:\n",
    "        if f in selected:\n",
    "            chosen_features[f] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(chosen_features) #Number of times features were in the top ten over 5 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking above, we can eliminate:\n",
    "- Year\n",
    "- Inflation\n",
    "- Unemployment\n",
    "- Life expectancy\n",
    "- Physicians\n",
    "\n",
    "Through trial, error and looking at permutation importance, I was also able to drop:\n",
    "- GDP (current US$) \n",
    "    - I investigated this as it can be calculated from population and GDP per capita\n",
    "- Educational attainment, Doctoral or equivalent, population 25+, total (%) (cumulative)\n",
    "    - showed negative permutation importance\n",
    "- Educational attainment, at least Master's or equivalent, population 25+, total (%) (cumulative)\n",
    "    - showed negative permutation importance\n",
    "- Educational attainment, at least completed short-cycle tertiary, population 25+, total (%) (cumulative)\n",
    "    - showed low permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = df.drop(['Inflation, consumer prices (annual %)',\n",
    "                          'Unemployment, total (% of total labor force) (modeled ILO estimate)',\n",
    "                          'Life expectancy at birth, total (years)',\n",
    "                          'Physicians (per 1,000 people)','GDP (current US$)',\"Educational attainment, at least Master's or equivalent, population 25+, total (%) (cumulative)\",\n",
    "                          \"Educational attainment, Doctoral or equivalent, population 25+, total (%) (cumulative)\",\n",
    "                          \"Educational attainment, at least completed short-cycle tertiary, population 25+, total (%) (cumulative)\"],axis=1)\n",
    "\n",
    "final_features.to_csv('../data/processed/selected_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
